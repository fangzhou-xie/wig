@book{villani2003,
  title = {Topics in Optimal Transportation},
  author = {Villani, Cédric},
  date = {2003-03-25},
  series = {Graduate {{Studies}} in {{Mathematics}}},
  volume = {58},
  publisher = {American Mathematical Society},
  doi = {10.1090/gsm/058},
  url = {http://www.ams.org/gsm/058},
  urldate = {2019-07-11},
  isbn = {978-0-8218-3312-4 978-0-8218-7232-1 978-1-4704-1804-5},
  langid = {english}
}

@article{knight2008,
  title = {The Sinkhorn-Knopp Algorithm: Convergence and Applications},
  shorttitle = {The {{Sinkhorn}}–{{Knopp Algorithm}}},
  author = {Knight, P.},
  date = {2008-01-01},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  volume = {30},
  number = {1},
  pages = {261--275},
  issn = {0895-4798},
  doi = {10.1137/060659624},
  url = {https://epubs.siam.org/doi/abs/10.1137/060659624},
  urldate = {2019-03-09},
  abstract = {As long as a square nonnegative matrix A contains sufficient nonzero elements, then the Sinkhorn–Knopp algorithm can be used to balance the matrix, that is, to find a diagonal scaling of A that is doubly stochastic. It is known that the convergence is linear, and an upper bound has been given for the rate of convergence for positive matrices. In this paper we give an explicit expression for the rate of convergence for fully indecomposable matrices. We describe how balancing algorithms can be used to give a measure of web page significance. We compare the measure with some well known alternatives, including PageRank. We show that, with an appropriate modification, the Sinkhorn–Knopp algorithm is a natural candidate for computing the measure on enormous data sets.}
}

@inproceedings{abadi2016,
  title = {{{TensorFlow}}: A System for Large-Scale Machine Learning},
  shorttitle = {{{TensorFlow}}},
  booktitle = {12th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 16)},
  author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2016},
  pages = {265--283},
  url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
  urldate = {2019-04-11}
}

@unpublished{alaya2019,
  title = {Screening Sinkhorn Algorithm for Regularized Optimal Transport},
  author = {Alaya, Mokhtar Z. and Bérar, Maxime and Gasso, Gilles and Rakotomamonjy, Alain},
  date = {2019-06-20},
  eprint = {1906.08540},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.08540},
  urldate = {2019-06-21},
  abstract = {We introduce in this paper a novel strategy for efficiently approximate the Sinkhorn distance between two discrete measures. After identifying neglectible components of the dual solution of the regularized Sinkhorn problem, we propose to screen those components by directly setting them at that value before entering the Sinkhorn problem. This allows us to solve a smaller Sinkhorn problem while ensuring approximation with provable guarantees. More formally, the approach is based on a reformulation of dual of Sinkhorn divergence problem and on the KKT optimality conditions of this problem, which enable identification of dual components to be screened. This new analysis leads to the Screenkhorn algorithm. We illustrate the efficiency of Screenkhorn on complex tasks such as dimensionality reduction or domain adaptation involving regularized optimal transport.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{cuturi2013,
  title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
  shorttitle = {Sinkhorn Distances},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Cuturi, Marco},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  date = {2013},
  pages = {2292--2300},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf},
  urldate = {2019-02-05}
}

@book{santambrogio2015a,
  title = {Optimal Transport for Applied Mathematicians: Calculus of Variations, {{PDEs}}, and Modeling},
  shorttitle = {Optimal Transport for Applied Mathematicians},
  author = {Santambrogio, Filippo},
  date = {2015-10-17},
  eprint = {UOHHCgAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Birkhäuser},
  abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
  isbn = {978-3-319-20828-2},
  langid = {english},
  pagetotal = {376},
  keywords = {Mathematics / Calculus,Mathematics / Differential Equations / General,Mathematics / Functional Analysis,Mathematics / General,Mathematics / Mathematical Analysis}
}

@book{villani2008,
  title = {Optimal Transport: Old and New},
  shorttitle = {Optimal Transport},
  author = {Villani, Cédric},
  date = {2008-10-26},
  eprint = {hV8o5R7_5tkC},
  eprinttype = {googlebooks},
  publisher = {Springer Science \& Business Media},
  abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results.   PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book’s value as a most welcome reference text on this subject.},
  isbn = {978-3-540-71050-9},
  langid = {english},
  pagetotal = {970},
  keywords = {Mathematics / Calculus,Mathematics / Differential Equations / General,Mathematics / Functional Analysis,Mathematics / Geometry / Differential,Mathematics / Mathematical Analysis}
}

@incollection{altschuler2017,
  title = {Near-Linear Time Approximation Algorithms for Optimal Transport via Sinkhorn Iteration},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {1964--1974},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6792-near-linear-time-approximation-algorithms-for-optimal-transport-via-sinkhorn-iteration.pdf},
  urldate = {2019-02-05}
}

@article{storn1997,
  title = {Differential Evolution – a Simple and Efficient Heuristic for Global Optimization over Continuous Spaces},
  author = {Storn, Rainer and Price, Kenneth},
  date = {1997-12-01},
  journaltitle = {Journal of Global Optimization},
  shortjournal = {Journal of Global Optimization},
  volume = {11},
  number = {4},
  pages = {341--359},
  issn = {1573-2916},
  doi = {10.1023/A:1008202821328},
  url = {https://doi.org/10.1023/A:1008202821328},
  urldate = {2020-11-16},
  abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
  langid = {english}
}

@article{xie2020,
  title = {Wasserstein Index Generation Model: Automatic Generation of Time-Series Index with Application to Economic Policy Uncertainty},
  shorttitle = {Wasserstein Index Generation Model},
  author = {Xie, Fangzhou},
  date = {2020-01-01},
  journaltitle = {Economics Letters},
  shortjournal = {Econ. Lett.},
  volume = {186},
  pages = {108874},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2019.108874},
  url = {http://www.sciencedirect.com/science/article/pii/S0165176519304410},
  urldate = {2019-12-10},
  abstract = {I propose a novel method, the Wasserstein Index Generation model (WIG), to generate a public sentiment index automatically. To test the model’s effectiveness, an application to generate Economic Policy Uncertainty (EPU) index is showcased.},
  langid = {english},
  selected = {true},
  keywords = {Economic Policy Uncertainty Index (EPU),Singular Value Decomposition (SVD),Wasserstein Dictionary Learning (WDL),Wasserstein Index Generation Model (WIG)}
}

@book{galichon2016,
  title = {Optimal Transport Methods in Economics},
  author = {Galichon, Alfred},
  date = {2016-08-23},
  eprint = {GDP9CwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Princeton University Press},
  doi = {10.23943/princeton/9780691172767.001.0001},
  abstract = {Optimal Transport Methods in Economics is the first textbook on the subject written especially for students and researchers in economics. Optimal transport theory is used widely to solve problems in mathematics and some areas of the sciences, but it can also be used to understand a range of problems in applied economics, such as the matching between job seekers and jobs, the determinants of real estate prices, and the formation of matrimonial unions. This is the first text to develop clear applications of optimal transport to economic modeling, statistics, and econometrics. It covers the basic results of the theory as well as their relations to linear programming, network flow problems, convex analysis, and computational geometry. Emphasizing computational methods, it also includes programming examples that provide details on implementation. Applications include discrete choice models, models of differential demand, and quantile-based statistical estimation methods, as well as asset pricing models.Authoritative and accessible, Optimal Transport Methods in Economics also features numerous exercises throughout that help you develop your mathematical agility, deepen your computational skills, and strengthen your economic intuition.The first introduction to the subject written especially for economistsIncludes programming examplesFeatures numerous exercises throughoutIdeal for students and researchers alike},
  isbn = {978-1-4008-8359-2},
  langid = {english},
  pagetotal = {185},
  keywords = {Business & Economics / Econometrics,Business & Economics / Economics / Microeconomics,Business & Economics / Economics / Theory,Business & Economics / Statistics,Mathematics / Applied}
}

@inproceedings{kingma2015,
  title = {Adam: A Method for Stochastic Optimization},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederick P and Ba, Jimmy},
  date = {2015}
}

@inproceedings{paszke2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  booktitle = {{{NIPS-W}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017}
}

@article{blei2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {993--1022},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v3/blei03a.html},
  urldate = {2019-01-10},
  issue = {Jan}
}

@incollection{xu2018,
  title = {Distilled Wasserstein Learning for Word Embedding and Topic Modeling},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Xu, Hongteng and Wang, Wenlin and Liu, Wei and Carin, Lawrence},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {1723--1732},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7443-distilled-wasserstein-learning-for-word-embedding-and-topic-modeling.pdf},
  urldate = {2019-01-13}
}

@inproceedings{dvurechensky2018,
  title = {Computational Optimal Transport: Complexity by Accelerated Gradient Descent Is Better than by Sinkhorn’s Algorithm},
  shorttitle = {Computational Optimal Transport},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Dvurechensky, Pavel and Gasnikov, Alexander and Kroshnin, Alexey},
  date = {2018-07-03},
  pages = {1367--1376},
  url = {http://proceedings.mlr.press/v80/dvurechensky18a.html},
  urldate = {2019-03-28},
  abstract = {We analyze two algorithms for approximating the general optimal transport (OT) distance between two discrete distributions of size \$n\$, up to accuracy \$\textbackslash varepsilon\$. For the first algorithm, which ...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@unpublished{leonard2013,
  title = {A Survey of the Schr\textbackslash "odinger Problem and Some of Its Connections with Optimal Transport},
  author = {Léonard, Christian},
  date = {2013-08-01},
  eprint = {1308.0215},
  eprinttype = {arXiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1308.0215},
  urldate = {2019-02-12},
  abstract = {This article is aimed at presenting the Schr\textbackslash "odinger problem and some of its connections with optimal transport. We hope that it can be used as a basic user's guide to Schr\textbackslash "odinger problem. We also give a survey of the related literature. In addition, some new results are proved.},
  keywords = {Mathematics - Functional Analysis,Mathematics - Optimization and Control,Mathematics - Probability}
}

@article{schmitz2018,
  title = {Wasserstein Dictionary Learning: Optimal Transport-Based Unsupervised Nonlinear Dictionary Learning},
  shorttitle = {Wasserstein Dictionary Learning},
  author = {Schmitz, Morgan A. and Heitz, Matthieu and Bonneel, Nicolas and Ngolè, Fred and Coeurjolly, David and Cuturi, Marco and Peyré, Gabriel and Starck, Jean-Luc},
  date = {2018-01},
  journaltitle = {SIAM Journal on Imaging Sciences},
  shortjournal = {SIAM J. Imaging Sci.},
  volume = {11},
  number = {1},
  pages = {643--678},
  issn = {1936-4954},
  doi = {10.1137/17M1140431},
  url = {https://epubs.siam.org/doi/10.1137/17M1140431},
  urldate = {2019-07-15},
  langid = {english}
}

@article{chizat2018,
  title = {Scaling Algorithms for Unbalanced Optimal Transport Problems},
  author = {Chizat, Lénaïc and Peyré, Gabriel and Schmitzer, Bernhard and Vialard, François-Xavier},
  date = {2018},
  journaltitle = {Mathematics of Computation},
  shortjournal = {Math. Comp.},
  volume = {87},
  number = {314},
  pages = {2563--2609},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/mcom/3303},
  url = {https://www.ams.org/mcom/2018-87-314/S0025-5718-2018-03303-8/},
  urldate = {2019-07-14},
  abstract = {This article introduces a new class of fast algorithms to approximate variational problems involving unbalanced optimal transport. While classical optimal transport considers only normalized probability distributions, it is important for many applications to be able to compute some sort of relaxed transportation between arbitrary positive measures. A generic class of such ``unbalanced'' optimal transport problems has been recently proposed by several authors. In this paper, we show how to extend the now classical entropic regularization scheme to these unbalanced problems. This gives rise to fast, highly parallelizable algorithms that operate by performing only diagonal scaling (i.e., pointwise multiplications) of the transportation couplings. They are generalizations of the celebrated Sinkhorn algorithm. We show how these methods can be used to solve unbalanced transport, unbalanced gradient flows, and to compute unbalanced barycenters. We showcase applications to 2-D shape modification, color transfer, and growth models.},
  langid = {english},
  keywords = {Bregman projections,Optimal transport,unbalanced transport,Wasserstein barycenters,Wasserstein distance}
}

@online{innes2019,
  title = {Don't Unroll Adjoint: Differentiating {{SSA-form}} Programs},
  shorttitle = {Don't Unroll Adjoint},
  author = {Innes, Michael},
  date = {2019-03-09},
  eprint = {1810.07951},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.07951},
  url = {http://arxiv.org/abs/1810.07951},
  urldate = {2022-07-27},
  abstract = {This paper presents reverse-mode algorithmic differentiation (AD) based on source code transformation, in particular of the Static Single Assignment (SSA) form used by modern compilers. The approach can support control flow, nesting, mutation, recursion, data structures, higher-order functions, and other language constructs, and the output is given to an existing compiler to produce highly efficient differentiated code. Our implementation is a new AD tool for the Julia language, called Zygote, which presents high-level dynamic semantics while transparently compiling adjoint code under the hood. We discuss the benefits of this approach to both the usability and performance of AD tools.},
  pubstate = {prepublished},
  keywords = {Computer Science - Programming Languages}
}

@article{monge1781,
  title = {Memoire Sur La Theorie Des Deblais et Des Remblais},
  author = {Monge, Gaspard},
  date = {1781},
  journaltitle = {Histoire de l'Academie Royale des Sciences de Paris},
  url = {https://ci.nii.ac.jp/naid/10018386702/},
  urldate = {2021-11-22}
}

@article{kantorovich1948,
  title = {On a Problem of Monge},
  author = {Kantorovich, L. V.},
  date = {1948},
  journaltitle = {C. R. (Doklady) Acad. Sci. URSS (N. S.)},
  volume = {3},
  pages = {225--226},
  url = {https://ci.nii.ac.jp/naid/10018386683/},
  urldate = {2021-12-08}
}

@article{kantorovich1942,
  title = {On the Translocation of Masses},
  author = {Kantorovich, L. V.},
  date = {1942},
  journaltitle = {Dokl. Akad. Nauk. USSR (N.S.)},
  volume = {37},
  pages = {199--201},
  url = {https://ci.nii.ac.jp/naid/10025352868/},
  urldate = {2021-12-08}
}

@book{nocedal2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen},
  date = {2006-07-27},
  edition = {2nd edition},
  publisher = {Springer},
  location = {New York},
  abstract = {Optimization is an important tool used in decision science and for the analysis of physical systems used in engineering. One can trace its roots to the Calculus of Variations and the work of Euler and Lagrange. This natural and reasonable approach to mathematical programming covers numerical methods for finite-dimensional optimization problems. It begins with very simple ideas progressing through more complicated concepts, concentrating on methods for both unconstrained and constrained optimization.},
  isbn = {978-0-387-30303-1},
  langid = {english},
  pagetotal = {686}
}

@article{fletcher1970,
  title = {A New Approach to Variable Metric Algorithms},
  author = {Fletcher, R.},
  date = {1970-01-01},
  journaltitle = {The Computer Journal},
  shortjournal = {The Computer Journal},
  volume = {13},
  number = {3},
  pages = {317--322},
  issn = {0010-4620},
  doi = {10.1093/comjnl/13.3.317},
  url = {https://doi.org/10.1093/comjnl/13.3.317},
  urldate = {2023-08-21},
  abstract = {An approach to variable metric algorithms has been investigated in which the linear search sub-problem no longer becomes necessary. The property of quadratic termination has been replaced by one of monotonic convergence of the eigenvalues of the approximating matrix to the inverse hessian. A convex class of updating formulae which possess this property has been established, and a strategy has been indicated for choosing a member of the class so as to keep the approximation away from both singularity and unboundedness. A FORTRAN program has been tested extensively with encouraging results.}
}

@article{broyden1970,
  title = {The Convergence of a Class of Double-Rank Minimization Algorithms 1. {{General}} Considerations},
  author = {Broyden, C. G.},
  date = {1970-03-01},
  journaltitle = {IMA Journal of Applied Mathematics},
  shortjournal = {IMA Journal of Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {76--90},
  issn = {0272-4960},
  doi = {10.1093/imamat/6.1.76},
  url = {https://doi.org/10.1093/imamat/6.1.76},
  urldate = {2023-08-21},
  abstract = {This paper presents a more detailed analysis of a class of minimization algorithms, which includes as a special case the DFP (Davidon-Fletcher-Powell) method, than has previously appeared. Only quadratic functions are considered but particular attention is paid to the magnitude of successive errors and their dependence upon the initial matrix. On the basis of this a possible explanation of some of the observed characteristics of the class is tentatively suggested.}
}

@article{goldfarb1970,
  title = {A Family of Variable-Metric Methods Derived by Variational Means},
  author = {Goldfarb, Donald},
  date = {1970},
  journaltitle = {Mathematics of Computation},
  volume = {24},
  number = {109},
  eprint = {2004873},
  eprinttype = {jstor},
  pages = {23--26},
  publisher = {American Mathematical Society},
  issn = {0025-5718},
  doi = {10.2307/2004873},
  url = {https://www.jstor.org/stable/2004873},
  urldate = {2023-08-21},
  abstract = {A new rank-two variable-metric method is derived using Greenstadt's variational approach [Math. Comp., this issue]. Like the Davidon-Fletcher-Powell (DFP) variable-metric method, the new method preserves the positive-definiteness of the approximating matrix. Together with Greenstadt's method, the new method gives rise to a one-parameter family of variable-metric methods that includes the DFP and rank-one methods as special cases. It is equivalent to Broyden's one-parameter family [Math. Comp., v. 21, 1967, pp. 368-381]. Choices for the inverse of the weighting matrix in the variational approach are given that lead to the derivation of the DFP and rank-one methods directly.}
}

@article{shanno1970,
  title = {Conditioning of Quasi-Newton Methods for Function Minimization},
  author = {Shanno, D. F.},
  date = {1970},
  journaltitle = {Mathematics of Computation},
  shortjournal = {Math. Comp.},
  volume = {24},
  number = {111},
  pages = {647--656},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/S0025-5718-1970-0274029-X},
  url = {https://www.ams.org/mcom/1970-24-111/S0025-5718-1970-0274029-X/},
  urldate = {2023-08-21},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  keywords = {conditioning of search methods,Function minimization,gradient search,Hessian matrix,inverse approximations,quasi-Newton methods,stability of search methods,steepest-descent methods,variable metric methods}
}

@article{liu1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C. and Nocedal, Jorge},
  date = {1989-08-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Mathematical Programming},
  volume = {45},
  number = {1},
  pages = {503--528},
  issn = {1436-4646},
  doi = {10.1007/BF01589116},
  url = {https://doi.org/10.1007/BF01589116},
  urldate = {2023-08-21},
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  langid = {english},
  keywords = {conjugate gradient method,Large scale nonlinear optimization,limited memory methods,partitioned quasi-Newton method}
}

@inproceedings{luise2018,
  title = {Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Luise, Giulia and Rudi, Alessandro and Pontil, Massimiliano and Ciliberto, Carlo},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html},
  urldate = {2023-11-06},
  abstract = {Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However, in most situations the  Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn approximation, proving that it enjoys the same smoothness as its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula allows to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.}
}

@inproceedings{janati2020,
  title = {Debiased Sinkhorn Barycenters},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
  date = {2020-11-21},
  pages = {4692--4701},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/janati20a.html},
  urldate = {2023-11-06},
  abstract = {Entropy regularization in optimal transport (OT) has been the driver of many recent interests for Wasserstein metrics and barycenters in machine learning. It allows to keep the appealing geometrical properties of the unregularized Wasserstein distance while having a significantly lower complexity thanks to Sinkhorn’s algorithm. However, entropy brings some inherent smoothing bias, resulting for example in blurred barycenters. This side effect has prompted an increasing temptation in the community to settle for a slower algorithm such as log-domain stabilized Sinkhorn which breaks the parallel structure that can be leveraged on GPUs, or even go back to unregularized OT. Here we show how this bias is tightly linked to the reference measure that defines the entropy regularizer and propose debiased Sinkhorn barycenters that preserve the best of worlds: fast Sinkhorn-like iterations without entropy smoothing. Theoretically, we prove that this debiasing is perfect for Gaussian distributions with equal variance. Empirically, we illustrate the reduced blurring and the computational advantage.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{shen2020,
  title = {Sinkhorn Barycenter via Functional Gradient Descent},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shen, Zebang and Wang, Zhenfu and Ribeiro, Alejandro and Hassani, Hamed},
  date = {2020},
  volume = {33},
  pages = {986--996},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/0a93091da5efb0d9d5649e7f6b2ad9d7-Abstract.html},
  urldate = {2023-11-06}
}

@article{lacombe2023,
  title = {Learning to Generate Wasserstein Barycenters},
  author = {Lacombe, Julien and Digne, Julie and Courty, Nicolas and Bonneel, Nicolas},
  date = {2023-04-01},
  journaltitle = {Journal of Mathematical Imaging and Vision},
  shortjournal = {J Math Imaging Vis},
  volume = {65},
  number = {2},
  pages = {354--370},
  issn = {1573-7683},
  doi = {10.1007/s10851-022-01121-y},
  url = {https://doi.org/10.1007/s10851-022-01121-y},
  urldate = {2023-11-06},
  abstract = {Optimal transport is a notoriously difficult problem to solve numerically, with current approaches often remaining intractable for very large-scale applications such as those encountered in machine learning. Wasserstein barycenters—the problem of finding measures in-between given input measures in the optimal transport sense—are even more computationally demanding as it requires to solve an optimization problem involving optimal transport distances. By training a deep convolutional neural network, we improve by a factor of 80 the computational speed of Wasserstein barycenters over the fastest state-of-the-art approach on the GPU, resulting in milliseconds computational times on \$\$512\textbackslash times 512\$\$regular grids. We show that our network, trained on Wasserstein barycenters of pairs of measures, generalizes well to the problem of finding Wasserstein barycenters of more than two measures. We demonstrate the efficiency of our approach for computing barycenters of sketches and transferring colors between multiple images.},
  langid = {english},
  keywords = {Color transfer,Convolutional neural network,Optimal transport,Wasserstein barycenter}
}

@inproceedings{genevay2018,
  title = {Learning Generative Models with Sinkhorn Divergences},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Genevay, Aude and Peyre, Gabriel and Cuturi, Marco},
  date = {2018-03-31},
  pages = {1608--1617},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v84/genevay18a.html},
  urldate = {2023-11-06},
  abstract = {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@inproceedings{cuturi2014a,
  title = {Fast Computation of Wasserstein Barycenters},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Cuturi, Marco and Doucet, Arnaud},
  date = {2014-06-18},
  pages = {685--693},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v32/cuturi14.html},
  urldate = {2023-11-06},
  abstract = {We present new algorithms to compute the mean of a set of 𝑁NN empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter~(Agueh and Carlier, 2011; Rabin et al, 2012), is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of Cuturi (2013), we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{lehmann2022,
  title = {A Note on Overrelaxation in the Sinkhorn Algorithm},
  author = {Lehmann, Tobias and family=Renesse, given=Max-K., prefix=von, useprefix=true and Sambale, Alexander and Uschmajew, André},
  date = {2022-11-01},
  journaltitle = {Optimization Letters},
  shortjournal = {Optim Lett},
  volume = {16},
  number = {8},
  pages = {2209--2220},
  issn = {1862-4480},
  doi = {10.1007/s11590-021-01830-0},
  url = {https://doi.org/10.1007/s11590-021-01830-0},
  urldate = {2023-11-08},
  abstract = {We derive an a priori parameter range for overrelaxation of the Sinkhorn algorithm, which guarantees global convergence and a strictly faster asymptotic local convergence. Guided by the spectral analysis of the linearized problem we pursue a zero cost procedure to choose a near optimal relaxation parameter.},
  langid = {english},
  keywords = {Matrix scaling,Optimal transport,Overrelaxation,Sinkhorn algorithm}
}

@article{shishkin2019,
  title = {Fast Approximate Truncated {{SVD}}},
  author = {Shishkin, Serge L. and Shalaginov, Arkadi and Bopardikar, Shaunak D.},
  date = {2019},
  journaltitle = {Numerical Linear Algebra with Applications},
  volume = {26},
  number = {4},
  pages = {e2246},
  issn = {1099-1506},
  doi = {10.1002/nla.2246},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2246},
  urldate = {2023-11-22},
  abstract = {This paper presents a new method for the computation of truncated singular value decomposition (SVD) of an arbitrary matrix. The method can be qualified as deterministic because it does not use randomized schemes. The number of operations required is asymptotically lower than that using conventional methods for nonsymmetric matrices and is at a par with the best existing deterministic methods for unstructured symmetric ones. It slightly exceeds the asymptotical computational cost of SVD methods based on randomization; however, the error estimate for such methods is significantly higher than for the presented one. The method is one-pass, that is, each value of the matrix is used just once. It is also readily parallelizable. In the case of full SVD decomposition, it is exact. In addition, it can be modified for a case when data are obtained sequentially rather than being available all at once. Numerical simulations confirm accuracy of the method.},
  langid = {english},
  keywords = {incremental computation,matrix factorization,scalability,singular value decomposition}
}

@article{larsen1998,
  title = {Lanczos Bidiagonalization with Partial Reorthogonalization},
  author = {Larsen, Rasmus Munk},
  date = {1998-12-01},
  journaltitle = {DAIMI Report Series},
  number = {537},
  issn = {2245-9316},
  doi = {10.7146/dpb.v27i537.7070},
  url = {https://tidsskrift.dk/daimipb/article/view/7070},
  urldate = {2023-11-22},
  abstract = {A partial reorthogonalization procedure (BPRO) for maintaining semi-orthogonality among the left and right Lanczos vectors in the Lanczos bidiagonalization (LBD) is presented. The resulting algorithm is mathematically equivalent to the symmetric Lanczos algorithm with partial reorthogonalization (PRO) developed by Simon but works directly on the Lanczos bidiagonalization of A. For computing the singular values and vectors of a large sparse matrix with high accuracy, the BPRO algorithm uses only half the amount of storage and a factor of 3-4 less work compared to methods based on PRO applied to an equivalent symmetric system. Like PRO the algorithm presented here is based on simple recurrences which enable it to monitor the loss of orthogonality among the Lanczos vectors directly without forming inner products.  These recurrences are used to develop a Lanczos bidiagonalization algorithm with partial reorthogonalization which has been implemented in a MATLAB package for sparse SVD and eigenvalue problems called PROPACK. Numerical experiments with the routines  from PROPACK are conducted using a test problem from inverse helioseismology to illustrate the properties of the method. In addition a number of test matrices from the Harwell-Boeing collection are used to compare the accuracy and efficiency of the MATLAB implementations of BPRO and PRO with the svds routine in MATLAB 5.1, which uses an implicitly restarted Lanczos algorithm.},
  issue = {537},
  langid = {english}
}

@article{bro2008,
  title = {Resolving the Sign Ambiguity in the Singular Value Decomposition},
  author = {Bro, R. and Acar, E. and Kolda, Tamara G.},
  date = {2008},
  journaltitle = {Journal of Chemometrics},
  volume = {22},
  number = {2},
  pages = {135--140},
  issn = {1099-128X},
  doi = {10.1002/cem.1122},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.1122},
  urldate = {2023-11-22},
  abstract = {Many modern data analysis methods involve computing a matrix singular value decomposition (SVD) or eigenvalue decomposition (EVD). Principal component analysis is the time-honored example, but more recent applications include latent semantic indexing (LSI), hypertext induced topic selection (HITS), clustering, classification, etc. Though the SVD and EVD are well established and can be computed via state-of-the-art algorithms, it is not commonly mentioned that there is an intrinsic sign indeterminacy that can significantly impact the conclusions and interpretations drawn from their results. Here we provide a solution to the sign ambiguity problem and show how it leads to more sensible solutions. Copyright © 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {PCA,sign indeterminacy,SVD}
}

@article{hansen1987,
  title = {The {{truncatedSVD}} as a Method for Regularization},
  author = {Hansen, Per Christian},
  date = {1987-12-01},
  journaltitle = {BIT Numerical Mathematics},
  shortjournal = {BIT},
  volume = {27},
  number = {4},
  pages = {534--553},
  issn = {1572-9125},
  doi = {10.1007/BF01937276},
  url = {https://doi.org/10.1007/BF01937276},
  urldate = {2023-11-22},
  abstract = {The truncated singular value decomposition (SVD) is considered as a method for regularization of ill-posed linear least squares problems. In particular, the truncated SVD solution is compared with the usual regularized solution. Necessary conditions are defined in which the two methods will yield similar results. This investigation suggests the truncated SVD as a favorable alternative to standard-form regularization in cases of ill-conditioned matrices with well-determined numerical rank.},
  langid = {english},
  keywords = {65F20,65F30,numerical rank,perturbation theory for truncated SVD,regularization in standard form,truncated SVD}
}

@article{chen2009,
  title = {Lanczos Vectors versus Singular Vectors for Effective Dimension Reduction},
  author = {Chen, Jie and Saad, Yousef},
  date = {2009-08},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {21},
  number = {8},
  pages = {1091--1103},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2008.228},
  url = {https://ieeexplore.ieee.org/abstract/document/4674352?casa_token=VXi6Ceh763MAAAAA:9bzyyM_rCAzCeMLnaO0rMdF3eiVZjOAtgzAefdzJoydeLKmGZfL_T6COwCH9MKqekmik-RtPat0},
  urldate = {2023-11-22},
  abstract = {This paper takes an in-depth look at a technique for computing filtered matrix-vector (mat-vec) products which are required in many data analysis applications. In these applications, the data matrix is multiplied by a vector and we wish to perform this product accurately in the space spanned by a few of the major singular vectors of the matrix. We examine the use of the Lanczos algorithm for this purpose. The goal of the method is identical with that of the truncated singular value decomposition (SVD), namely to preserve the quality of the resulting mat-vec product in the major singular directions of the matrix. The Lanczos-based approach achieves this goal by using a small number of Lanczos vectors, but it does not explicitly compute singular values/vectors of the matrix. The main advantage of the Lanczos-based technique is its low cost when compared with that of the truncated SVD. This advantage comes without sacrificing accuracy. The effectiveness of this approach is demonstrated on a few sample applications requiring dimension reduction, including information retrieval and face recognition. The proposed technique can be applied as a replacement to the truncated SVD technique whenever the problem can be formulated as a filtered mat-vec multiplication.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}}
}

@article{halko2011,
  title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
  shorttitle = {Finding Structure with Randomness},
  author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
  date = {2011-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {53},
  number = {2},
  pages = {217--288},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/090771806},
  url = {https://epubs.siam.org/doi/abs/10.1137/090771806},
  urldate = {2023-11-23},
  abstract = {This paper describes a suite of algorithms for constructing low-rank approximations of an input matrix from a random linear image, or sketch, of the matrix. These methods can preserve structural properties of the input matrix, such as positive-semidefiniteness, and they can produce approximations with a user-specified rank. The algorithms are simple, accurate, numerically stable, and provably correct. Moreover, each method is accompanied by an informative error bound that allows users to select parameters a priori to achieve a given approximation quality. These claims are supported by numerical experiments with real and synthetic data.}
}

@online{boulle2022,
  title = {A Generalization of the Randomized Singular Value Decomposition},
  author = {Boullé, Nicolas and Townsend, Alex},
  date = {2022-01-21},
  eprint = {2105.13052},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2105.13052},
  url = {http://arxiv.org/abs/2105.13052},
  urldate = {2023-11-23},
  abstract = {The randomized singular value decomposition (SVD) is a popular and effective algorithm for computing a near-best rank \$k\$ approximation of a matrix \$A\$ using matrix-vector products with standard Gaussian vectors. Here, we generalize the randomized SVD to multivariate Gaussian vectors, allowing one to incorporate prior knowledge of \$A\$ into the algorithm. This enables us to explore the continuous analogue of the randomized SVD for Hilbert--Schmidt (HS) operators using operator-function products with functions drawn from a Gaussian process (GP). We then construct a new covariance kernel for GPs, based on weighted Jacobi polynomials, which allows us to rapidly sample the GP and control the smoothness of the randomly generated functions. Numerical examples on matrices and HS operators demonstrate the applicability of the algorithm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@inproceedings{feng2018,
  title = {Faster Matrix Completion Using Randomized {{SVD}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Feng, Xu and Yu, Wenjian and Li, Yaohang},
  date = {2018-11},
  pages = {608--615},
  issn = {2375-0197},
  doi = {10.1109/ICTAI.2018.00098},
  url = {https://ieeexplore.ieee.org/abstract/document/8576096?casa_token=at19m5poBt8AAAAA:p2BQOXywnEZWMl2Hj3h-OctQRCOaR6D9wgZ9k5QLYzVySVXUwdVFL-buoCWYa8k0DfinN9jmNJk},
  urldate = {2023-11-23},
  abstract = {Matrix completion is a widely used technique for image inpainting and personalized recommender system, etc. In this work, we focus on accelerating the matrix completion using faster randomized singular value decomposition (rSVD). Firstly, two fast randomized algorithms (rSVD-PI and rSVDBKI) are proposed for handling sparse matrix. They make use of an eigSVD procedure and several accelerating skills. Then, with the rSVD-BKI algorithm and a new subspace recycling technique, we accelerate the singular value thresholding (SVT) method in [1] to realize faster matrix completion. Experiments show that the proposed rSVD algorithms can be 6× faster than the basic rSVD algorithm [2] while keeping same accuracy. For image inpainting and movie-rating estimation problems (including up to 2 × 107 ratings), the proposed accelerated SVT algorithm consumes 15× and 8× less CPU time than the methods using svds and lansvd respectively, without loss of accuracy.},
  eventtitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})}
}

@article{benamou2015,
  title = {Iterative Bregman Projections for Regularized Transportation Problems},
  author = {Benamou, Jean-David and Carlier, Guillaume and Cuturi, Marco and Nenna, Luca and Peyré, Gabriel},
  date = {2015-01},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {37},
  number = {2},
  pages = {A1111-A1138},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/141000439},
  url = {https://epubs.siam.org/doi/abs/10.1137/141000439},
  urldate = {2024-01-08},
  abstract = {This article reviews the use of first order convex optimization schemes to solve the discretized dynamic optimal transport problem, initially proposed by Benamou and Brenier. We develop a staggered grid discretization that is well adapted to the computation of the \$L\textasciicircum 2\$ optimal transport geodesic between distributions defined on a uniform spatial grid. We show how proximal splitting schemes can be used to solve the resulting large scale convex optimization problem. A specific instantiation of this method on a centered grid corresponds to the initial algorithm developed by Benamou and Brenier. We also show how more general cost functions can be taken into account and how to extend the method to perform optimal transport on a Riemannian manifold.}
}

@inproceedings{pooladian2022,
  title = {Debiaser Beware: Pitfalls of Centering Regularized Transport Maps},
  shorttitle = {Debiaser Beware},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Pooladian, Aram-Alexandre and Cuturi, Marco and Niles-Weed, Jonathan},
  date = {2022-06-28},
  pages = {17830--17847},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/pooladian22a.html},
  urldate = {2024-02-06},
  abstract = {Estimating optimal transport (OT) maps (a.k.a. Monge maps) between two measures P and Q is a problem fraught with computational and statistical challenges. A promising approach lies in using the dual potential functions obtained when solving an entropy-regularized OT problem between samples P\_n and Q\_n, which can be used to recover an approximately optimal map. The negentropy penalization in that scheme introduces, however, an estimation bias that grows with the regularization strength. A well-known remedy to debias such estimates, which has gained wide popularity among practitioners of regularized OT, is to center them, by subtracting auxiliary problems involving P\_n and itself, as well as Q\_n and itself. We do prove that, under favorable conditions on P and Q, debiasing can yield better approximations to the Monge map. However, and perhaps surprisingly, we present a few cases in which debiasing is provably detrimental in a statistical sense, notably when the regularization strength is large or the number of samples is small. These claims are validated experimentally on synthetic and real datasets, and should reopen the debate on whether debiasing is needed when using entropic OT.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{schmitzer2019,
  title = {Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems},
  author = {Schmitzer, Bernhard},
  date = {2019-01},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {41},
  number = {3},
  pages = {A1443-A1481},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/16M1106018},
  url = {https://epubs.siam.org/doi/abs/10.1137/16M1106018},
  urldate = {2024-02-06},
  abstract = {This paper details a general numerical framework to approximate solutions to linear programs related to optimal transport. The general idea is to introduce an entropic regularization of the initial linear program. This regularized problem corresponds to a  Kullback--Leibler Bregman divergence projection of a vector (representing some initial joint distribution) on the polytope of constraints. We show that for many problems related to optimal transport, the set of linear constraints can be split in an intersection of a few simple constraints, for which the projections can be computed in closed form. This allows us to make use of iterative Bregman projections (when there are only equality constraints) or, more generally, Bregman--Dykstra iterations (when  inequality constraints are involved). We illustrate the usefulness of this approach for several variational problems related to optimal transport: barycenters for the optimal transport metric, tomographic reconstruction, multimarginal optimal transport, and in particular its application to Brenier's relaxed solutions of incompressible Euler equations, partial unbalanced optimal transport, and optimal transport with capacity constraints.}
}

@book{magnus2019,
  title = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
  author = {Magnus, Jan R. and Neudecker, Heinz},
  date = {2019-03-18},
  edition = {3rd edition},
  publisher = {Wiley},
  location = {Hoboken (N.J.)},
  abstract = {A brand new, fully updated edition of a popular classic on matrix differential calculus with applications in statistics and econometricsThis exhaustive, self-contained book on matrix theory and matrix differential calculus provides a treatment of matrix calculus based on differentials and shows how easy it is to use this theory once you have mastered the technique. Jan Magnus, who, along with the late Heinz Neudecker, pioneered the theory, develops it further in this new edition and provides many examples along the way to support it.Matrix calculus has become an essential tool for quantitative methods in a large number of applications, ranging from social and behavioral sciences to econometrics. It is still relevant and used today in a wide range of subjects such as the biosciences and psychology. Matrix Differential Calculus with Applications in Statistics and Econometrics, Third Edition contains all of the essentials of multivariable calculus with an emphasis on the use of differentials. It starts by presenting a concise, yet thorough overview of matrix algebra, then goes on to develop the theory of differentials. The rest of the text combines the theory and application of matrix differential calculus, providing the practitioner and researcher with both a quick review and a detailed reference.Fulfills the need for an updated and unified treatment of matrix differential calculusContains many new examples and exercises based on questions asked of the author over the yearsCovers new developments in field and features new applicationsWritten by a leading expert and pioneer of the theoryPart of the Wiley Series in Probability and Statistics~Matrix Differential Calculus With Applications in Statistics and Econometrics Third Edition is an ideal text for graduate students and academics studying the subject, as well as for postgraduates and specialists working in biosciences and psychology.},
  isbn = {978-1-119-54120-2},
  langid = {english},
  pagetotal = {504}
}

@article{petersen2008,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  date = {2008},
  journaltitle = {Technical University of Denmark},
  shortjournal = {Technical University of Denmark},
  volume = {7},
  number = {15},
  pages = {510}
}

@online{karcher2014,
  title = {Riemannian {{Center}} of {{Mass}} and so Called Karcher Mean},
  author = {Karcher, Hermann},
  date = {2014-07-03},
  eprint = {1407.2087},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1407.2087},
  url = {http://arxiv.org/abs/1407.2087},
  urldate = {2024-07-28},
  abstract = {The Riemannian center of mass was constructed in [GrKa] (1973). In [GKR1, GKR2, Gr, Ka, BuKa] (1974-1981) it was successfully applied with more refined estimates. Probably in 1990 someone renamed it without justification into karcher mean and references to the older papers were omitted by those using the new name. As a consequence newcomers started to reprove results from the above papers. - Here I explain the older history.},
  pubstate = {prepublished},
  keywords = {53C20 01A60,Mathematics - Differential Geometry,Mathematics - History and Overview}
}

@inproceedings{gramfort2015,
  title = {Fast {{Optimal Transport Averaging}} of {{Neuroimaging Data}}},
  booktitle = {Information {{Processing}} in {{Medical Imaging}}},
  author = {Gramfort, A. and Peyré, G. and Cuturi, M.},
  editor = {Ourselin, Sebastien and Alexander, Daniel C. and Westin, Carl-Fredrik and Cardoso, M. Jorge},
  date = {2015},
  pages = {261--272},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-19992-4_20},
  abstract = {Knowing how the Human brain is anatomically and functionally organized at the level of a group of healthy individuals or patients is the primary goal of neuroimaging research. Yet computing an average of brain imaging data defined over a voxel grid or a triangulation remains a challenge. Data are large, the geometry of the brain is complex and the between subjects variability leads to spatially or temporally non-overlapping effects of interest. To address the problem of variability, data are commonly smoothed before performing a linear group averaging. In this work we build on ideas originally introduced by Kantorovich~[18] to propose a new algorithm that can average efficiently non-normalized data defined over arbitrary discrete domains using transportation metrics. We show how Kantorovich means can be linked to Wasserstein barycenters in order to take advantage of the entropic smoothing approach used by~[7]. It leads to a smooth convex optimization problem and an algorithm with strong convergence guarantees. We illustrate the versatility of this tool and its empirical behavior on functional neuroimaging data, functional MRI and magnetoencephalography (MEG) source estimates, defined on voxel grids and triangulations of the folded cortical surface.},
  isbn = {978-3-319-19992-4},
  langid = {english}
}

@article{cuturi2016,
  title = {A {{Smoothed Dual Approach}} for {{Variational Wasserstein Problems}}},
  author = {Cuturi, Marco and Peyré, Gabriel},
  date = {2016-01},
  journaltitle = {SIAM Journal on Imaging Sciences},
  shortjournal = {SIAM J. Imaging Sci.},
  volume = {9},
  number = {1},
  pages = {320--343},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/15M1032600},
  url = {https://epubs.siam.org/doi/abs/10.1137/15M1032600},
  urldate = {2024-07-28},
  abstract = {This paper details a general numerical framework to approximate solutions to linear programs related to optimal transport. The general idea is to introduce an entropic regularization of the initial linear program. This regularized problem corresponds to a  Kullback--Leibler Bregman divergence projection of a vector (representing some initial joint distribution) on the polytope of constraints. We show that for many problems related to optimal transport, the set of linear constraints can be split in an intersection of a few simple constraints, for which the projections can be computed in closed form. This allows us to make use of iterative Bregman projections (when there are only equality constraints) or, more generally, Bregman--Dykstra iterations (when  inequality constraints are involved). We illustrate the usefulness of this approach for several variational problems related to optimal transport: barycenters for the optimal transport metric, tomographic reconstruction, multimarginal optimal transport, and in particular its application to Brenier's relaxed solutions of incompressible Euler equations, partial unbalanced optimal transport, and optimal transport with capacity constraints.}
}

@article{peyre2019,
  title = {Computational {{Optimal Transport}}: {{With Applications}} to {{Data Science}}},
  shorttitle = {Computational {{Optimal Transport}}},
  author = {Peyré, Gabriel and Cuturi, Marco},
  date = {2019-02-11},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  volume = {11},
  number = {5-6},
  pages = {355--607},
  publisher = {Now Publishers, Inc.},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000073},
  url = {https://www.nowpublishers.com/article/Details/MAL-073},
  urldate = {2024-07-29},
  abstract = {Computational Optimal Transport: With Applications to Data Science},
  langid = {english}
}

@incollection{blei2009,
  title = {Topic {{Models}}},
  booktitle = {Text {{Mining}}},
  author = {Blei, David M. and Lafferty, John D.},
  date = {2009},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly faced with access to millions of articles in her field, is not satisfied with simple search. Effectively using such collections requires interacting with them in a more structured way: finding articles similar to those of interest, and exploring the collection through the underlying topics that run through it.},
  isbn = {978-0-429-19198-5},
  pagetotal = {24}
}

@misc{leal2018,
  title = {Autodiff, a Modern, Fast and Expressive {{C}}++ Library for Automatic Differentiation},
  author = {Leal, Allan M. M.},
  date = {2018},
  url = {https://autodiff.github.io},
  howpublished = {{\url{https://autodiff.github.io}}}
}

@online{loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2024-08-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control}
}

@inproceedings{xie2020a,
  title = {Pruned {{Wasserstein Index Generation Model}} and Wigpy {{Package}}},
  booktitle = {{{CARMA}} 2020 - 3rd {{International Conference}} on {{Advanced Research Methods}} and {{Analytics}}},
  author = {Xie, Fangzhou},
  date = {2020-07-08},
  eprint = {2004.00999},
  eprinttype = {arXiv},
  eprintclass = {cs, econ, q-fin},
  doi = {10.4995/CARMA2020.2020.11557},
  url = {http://arxiv.org/abs/2004.00999},
  urldate = {2024-08-02},
  abstract = {Recent proposal of Wasserstein Index Generation model (WIG) has shown a new direction for automatically generating indices. However, it is challenging in practice to fit large datasets for two reasons. First, the Sinkhorn distance is notoriously expensive to compute and suffers from dimensionality severely. Second, it requires to compute a full \$N\textbackslash times N\$ matrix to be fit into memory, where \$N\$ is the dimension of vocabulary. When the dimensionality is too large, it is even impossible to compute at all. I hereby propose a Lasso-based shrinkage method to reduce dimensionality for the vocabulary as a pre-processing step prior to fitting the WIG model. After we get the word embedding from Word2Vec model, we could cluster these high-dimensional vectors by \$k\$-means clustering, and pick most frequent tokens within each cluster to form the "base vocabulary". Non-base tokens are then regressed on the vectors of base token to get a transformation weight and we could thus represent the whole vocabulary by only the "base tokens". This variant, called pruned WIG (pWIG), will enable us to shrink vocabulary dimension at will but could still achieve high accuracy. I also provide a \textbackslash textit\{wigpy\} module in Python to carry out computation in both flavor. Application to Economic Policy Uncertainty (EPU) index is showcased as comparison with existing methods of generating time-series sentiment indices.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Economics - General Economics}
}

@article{lin2022,
  title = {On the {{Efficiency}} of {{Entropic Regularized Algorithms}} for {{Optimal Transport}}},
  author = {Lin, Tianyi and Ho, Nhat and Jordan, Michael I.},
  date = {2022},
  journaltitle = {Journal of Machine Learning Research},
  volume = {23},
  number = {137},
  pages = {1--42},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v23/20-277.html},
  urldate = {2024-08-02},
  abstract = {We present several new complexity results for the entropic regularized algorithms that approximately solve the optimal transport (OT) problem between two discrete probability measures with at most  n n  atoms. First, we improve the complexity bound of a greedy variant of Sinkhorn, known as Greenkhorn, from  O ̃~ ( n 2 ε −3 ) O\textasciitilde (n2ε−3)  to  O ̃~ ( n 2 ε −2 ) O\textasciitilde (n2ε−2) . Notably, our result can match the best known complexity bound of Sinkhorn and help clarify why Greenkhorn significantly outperforms Sinkhorn in practice in terms of row/column updates as observed by Altschuler et al. (2017). Second, we propose a new algorithm, which we refer to as APDAMD and which generalizes an adaptive primal-dual accelerated gradient descent (APDAGD) algorithm (Dvurechensky et al., 2018) with a prespecified mirror mapping  ϕ ϕ . We prove that APDAMD achieves the complexity bound of  O ̃~ ( n 2 δ √ ε −1 ) O\textasciitilde (n2δε−1)  in which  δ{$>$}0 δ{$>$}0  stands for the regularity of  ϕ ϕ . In addition, we show by a counterexample that the complexity bound of  O ̃~ (min\{ n 9/4 ε −1 , n 2 ε −2 \}) O\textasciitilde (min\{n9/4ε−1,n2ε−2\})  proved for APDAGD before is invalid and give a refined complexity bound of  O ̃~ ( n 5/2 ε −1 ) O\textasciitilde (n5/2ε−1) . Further, we develop a deterministic accelerated variant of Sinkhorn via appeal to estimated sequence and prove the complexity bound of  O ̃~ ( n 7/3 ε −4/3 ) O\textasciitilde (n7/3ε−4/3) . As such, we see that accelerated variant of Sinkhorn outperforms Sinkhorn and Greenkhorn in terms of  1/ε 1/ε  and APDAGD and accelerated alternating minimization (AAM) (Guminov et al., 2021) in terms of  n n . Finally, we conduct the experiments on synthetic and real data and the numerical results show the efficiency of Greenkhorn, APDAMD and accelerated Sinkhorn in practice.}
}

@article{sinkhorn1967,
  title = {Concerning Nonnegative Matrices and Doubly Stochastic Matrices},
  author = {Sinkhorn, Richard and Knopp, Paul},
  date = {1967-05-01},
  journaltitle = {Pacific Journal of Mathematics},
  volume = {21},
  number = {2},
  pages = {343--348},
  publisher = {Mathematical Sciences Publishers},
  issn = {0030-8730},
  url = {https://msp.org/pjm/1967/21-2/p14.xhtml},
  urldate = {2024-08-03}
}

@article{sinkhorn1964,
  title = {A {{Relationship Between Arbitrary Positive Matrices}} and {{Doubly Stochastic Matrices}}},
  author = {Sinkhorn, Richard},
  date = {1964-06},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {35},
  number = {2},
  pages = {876--879},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177703591},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-2/A-Relationship-Between-Arbitrary-Positive-Matrices-and-Doubly-Stochastic-Matrices/10.1214/aoms/1177703591.full},
  urldate = {2024-08-03},
  abstract = {The Annals of Mathematical Statistics}
}

@book{schrodinger1931,
  title = {Über Die Umkehrung Der Naturgesetze},
  author = {Schrödinger, Erwin},
  date = {1931},
  publisher = {Verlag der Akademie der Wissenschaften in Kommission bei Walter De Gruyter u~…}
}

@book{bacharach1970,
  title = {Biproportional {{Matrices}} and {{Input-Output Change}}},
  author = {Bacharach, Michael},
  date = {1970-05-02},
  eprint = {jfM8AAAAIAAJ},
  eprinttype = {googlebooks},
  publisher = {CUP Archive},
  abstract = {Study of mathematical analysis and statistical methods of estimating input output changes over time. Bibliography pp. 167 to 170.},
  isbn = {978-0-521-07594-7},
  langid = {english},
  pagetotal = {192},
  keywords = {Business & Economics / Econometrics,Business & Economics / Economics / General,Business & Economics / Economics / Macroeconomics}
}

@online{modin2024,
  title = {On the Geometry and Dynamical Formulation of the {{Sinkhorn}} Algorithm for Optimal Transport},
  author = {Modin, Klas},
  date = {2024-01-24},
  eprint = {2309.09089},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2309.09089},
  url = {http://arxiv.org/abs/2309.09089},
  urldate = {2024-08-03},
  abstract = {The Sinkhorn algorithm is a numerical method for the solution of optimal transport problems. Here, I give a brief survey of this algorithm, with a strong emphasis on its geometric origin: it is natural to view it as a discretization, by standard methods, of a non-linear integral equation. In the appendix, I also provide a short summary of an early result of Beurling on product measures, directly related to the Sinkhorn algorithm.},
  pubstate = {prepublished},
  keywords = {49Q22 35Q49 37K65 65R20,Mathematics - Differential Geometry,Mathematics - Numerical Analysis}
}

@article{dekel2012,
  title = {Optimal {{Distributed Online Prediction Using Mini-Batches}}},
  author = {Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {Journal of Machine Learning Research},
  volume = {13},
  number = {1},
  issn = {1532-4435}
}

@article{cauchy1847,
  title = {Méthode Générale Pour La Résolution Des Systemes d’équations Simultanées},
  author = {Cauchy, Augustin},
  date = {1847},
  journaltitle = {Comp. Rend. Sci. Paris},
  shortjournal = {Comp. Rend. Sci. Paris},
  volume = {25},
  number = {1847},
  pages = {536--538}
}

@article{lemarechal2012,
  title = {Cauchy and the Gradient Method},
  author = {Lemaréchal, Claude},
  date = {2012},
  journaltitle = {Doc Math Extra},
  shortjournal = {Doc Math Extra},
  volume = {251},
  number = {254},
  pages = {10}
}

@book{hadamard1908,
  title = {Mémoire Sur Le Problème d'analyse Relatif à l'équilibre Des Plaques Élastiques Encastrées},
  author = {Hadamard, Jacques},
  date = {1908},
  volume = {33},
  publisher = {Imprimerie nationale}
}

@article{courant1943,
  title = {Variational Methods for the Solution of Problems of Equilibrium and Vibrations},
  author = {Courant, R.},
  date = {1943},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  volume = {49},
  number = {1},
  pages = {1--23},
  issn = {0002-9904, 1936-881X},
  doi = {10.1090/S0002-9904-1943-07818-4},
  url = {https://www.ams.org/bull/1943-49-01/S0002-9904-1943-07818-4/},
  urldate = {2024-08-08},
  abstract = {Advancing research. Creating connections.},
  langid = {english}
}

@article{hilbert1900,
  title = {Sur Le Principe de {{Dirichlet}}},
  author = {Hilbert, David},
  date = {1900},
  journaltitle = {Nouvelles annales de mathématiques: journal des candidats aux écoles polytechnique et normale},
  shortjournal = {Nouvelles annales de mathématiques: journal des candidats aux écoles polytechnique et normale},
  volume = {19},
  pages = {337--344},
  issn = {2400-4782}
}

@book{rayleigh1896,
  title = {The {{Theory}} of {{Sound}}},
  author = {Rayleigh, John William Strutt Baron},
  date = {1896},
  eprint = {A7fvAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {Macmillan},
  langid = {english},
  pagetotal = {526}
}

@article{ritz1909,
  title = {Über eine neue Methode zur Lösung gewisser Variationsprobleme der mathematischen Physik.},
  author = {Ritz, Walter},
  date = {1909},
  journaltitle = {Journal für die reine und angewandte Mathematik},
  volume = {135},
  pages = {1--61},
  url = {http://eudml.org/doc/149295},
  langid = {german},
  keywords = {Dirichlet principle,elastic plate,Ritz method}
}

@article{robbins1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  date = {1951-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full},
  urldate = {2024-08-08},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.}
}

@article{rosenblatt1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1471},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Brain,Cognition,Memory,Nervous System}
}

@article{kiefer1952,
  title = {Stochastic {{Estimation}} of the {{Maximum}} of a {{Regression Function}}},
  author = {Kiefer, J. and Wolfowitz, J.},
  date = {1952-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {3},
  pages = {462--466},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729392},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-3/Stochastic-Estimation-of-the-Maximum-of-a-Regression-Function/10.1214/aoms/1177729392.full},
  urldate = {2024-08-09},
  abstract = {Let \$M(x)\$ be a regression function which has a maximum at the unknown point \$\textbackslash theta. M(x)\$ is itself unknown to the statistician who, however, can take observations at any level \$x\$. This paper gives a scheme whereby, starting from an arbitrary point \$x\_1\$, one obtains successively \$x\_2, x\_3, \textbackslash cdots\$ such that \$x\_n\$ converges to \$\textbackslash theta\$ in probability as \$n \textbackslash rightarrow \textbackslash infty\$.}
}

@article{peng2020b,
  title = {Accelerating {{Minibatch Stochastic Gradient Descent Using Typicality Sampling}}},
  author = {Peng, Xinyu and Li, Li and Wang, Fei-Yue},
  date = {2020-11},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {11},
  pages = {4649--4659},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2957003},
  url = {https://ieeexplore.ieee.org/abstract/document/8945166},
  urldate = {2024-08-09},
  abstract = {Machine learning, especially deep neural networks, has developed rapidly in fields, including computer vision, speech recognition, and reinforcement learning. Although minibatch stochastic gradient descent (SGD) is one of the most popular stochastic optimization methods for training deep networks, it shows a slow convergence rate due to the large noise in the gradient approximation. In this article, we attempt to remedy this problem by building a more efficient batch selection method based on typicality sampling, which reduces the error of gradient estimation in conventional minibatch SGD. We analyze the convergence rate of the resulting typical batch SGD algorithm and compare the convergence properties between the minibatch SGD and the algorithm. Experimental results demonstrate that our batch selection scheme works well and more complex minibatch SGD variants can benefit from the proposed batch selection strategy.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Acceleration,Approximation algorithms,Batch selection,Convergence,Estimation,machine learning,minibatch stochastic gradient descent (SGD),Optimization,speed of convergence,Stochastic processes,Training}
}

@inproceedings{li2014a,
  title = {Efficient Mini-Batch Training for Stochastic Optimization},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J.},
  date = {2014-08-24},
  series = {{{KDD}} '14},
  pages = {661--670},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2623330.2623612},
  url = {https://dl.acm.org/doi/10.1145/2623330.2623612},
  urldate = {2024-08-08},
  abstract = {Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.},
  isbn = {978-1-4503-2956-9}
}
